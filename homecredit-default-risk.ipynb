{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HOME CREDIT DEFAULT RISK\n",
    "\n",
    "######################################\n",
    "# Kütüphanelerin import edilmesi\n",
    "#####################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import re\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "    \n",
    "#######################\n",
    "# Fonksiyonlar\n",
    "#######################\n",
    "\n",
    "# veri setine genel bakış\n",
    "def check_df(dataframe):\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    \n",
    "# Değişkenlerin türlerinin belirlenmesi\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    # cat_cols, cat_but_car\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    #print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    #print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    #print(f'cat_cols: {len(cat_cols)}')\n",
    "    #print(f'num_cols: {len(num_cols)}')\n",
    "    #print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    #print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, num_cols, cat_but_car\n",
    "\n",
    "# Eksik gözlemler\n",
    "def missing_values_table(dataframe, na_name=False):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    print(missing_df, end=\"\\n\")\n",
    "    if na_name:\n",
    "        return na_columns\n",
    "    \n",
    "# Kategorik değişkenlerin incelenmesi\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "    print(\"##########################################\")\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.show()\n",
    "        \n",
    "# Nümerik değişkenlerin incelenmesi\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=20)\n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# Korelasyonlar\n",
    "def high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n",
    "    if len(remove) > 0:\n",
    "        cols = [x for x in data.columns if (x not in remove)]\n",
    "        c = data[cols].corr(method=corr_coef)\n",
    "    else:\n",
    "        c = data.corr(method=corr_coef)\n",
    "\n",
    "    for i in c.columns:\n",
    "        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n",
    "        if len(cr) > 0:\n",
    "            print(i)\n",
    "            print(\"-------------------------------\")\n",
    "            print(cr.sort_values(ascending=False))\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Rare Encoding\n",
    "def rare_encoder(dataframe, rare_perc, cat_cols):\n",
    "   \n",
    "    rare_columns = [col for col in cat_cols if\n",
    "                    (dataframe[col].value_counts() / len(dataframe) < rare_perc).sum() > 1]\n",
    "\n",
    "    for col in rare_columns:\n",
    "        tmp = dataframe[col].value_counts() / len(dataframe)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# One-hot Encoding \n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "#########################\n",
    "# Application_train_test\n",
    "#########################\n",
    "\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Veri setinin okutulması\n",
    "    df = pd.read_csv('../input/home-credit-default-risk/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    \n",
    "    # Cinsiyeti belirtilmeyen 4 kişi var bunları çıkarıyoruz.\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    # Medeni durumu unknown olan 1 kişi var bunu dropladık.\n",
    "    df = df[df['NAME_FAMILY_STATUS'] != \"Unknown\" ]\n",
    "    # NaN values for DAYS_EMPLOYED: 365243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    ########\n",
    "    # RARE\n",
    "    ########\n",
    "    # NAME_INCOME_TYPE değişkeninin 4 sınıfının frekansı diğerlerine göre düşük olduğunu gözlemledik.\n",
    "    # Bu nedenle bu 4 sınıfı kendilerine en yakın olabilecek sınıfın için dahil ettik.\n",
    "    # Yani rare sınıfını diğerlerine eklemiş olduk.\n",
    "    df.loc[df['NAME_INCOME_TYPE'] == 'Businessman', 'NAME_INCOME_TYPE'] = 'Commercial associate'\n",
    "    df.loc[df['NAME_INCOME_TYPE'] == 'Maternity leave', 'NAME_INCOME_TYPE'] = 'Pensioner'\n",
    "    df.loc[df['NAME_INCOME_TYPE'] == 'Student', 'NAME_INCOME_TYPE'] = 'State servant'\n",
    "    df.loc[df['NAME_INCOME_TYPE'] == 'Unemployed', 'NAME_INCOME_TYPE'] = 'Pensioner'\n",
    "    \n",
    "    # ORGANIZATION_TYPE\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Business Entity\"), \n",
    "                                       \"Business_Entity\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Industry\"), \n",
    "                                       \"Industry\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Trade\"),\n",
    "                                       \"Trade\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Transport\"),\n",
    "                                       \"Transport\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"School\", \"Kindergarten\", \"University\"]),\n",
    "                                       \"Education\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Official\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Bank\", \"Insurance\"]),\n",
    "                                       \"Finance\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Goverment\"), \n",
    "                                       \"Government\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Realtor\", \"Housing\"]), \"Realty\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Hotel\", \"Restaurant\",\"Services\"]), \"TourismFoodSector\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df[\"ORGANIZATION_TYPE\"])\n",
    "    \n",
    "    \n",
    "    # OCCUPATION_TYPE\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters/barmen staff\"]), \"Low_skill_staff\", df[\"OCCUPATION_TYPE\"])\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"IT staff\", \"High skill tech staff\"]), \"High_skill_staff\", df[\"OCCUPATION_TYPE\"])\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Secretaries\", \"HR staff\",\"Realty agents\"]), \"Others\", df[\"OCCUPATION_TYPE\"])\n",
    "\n",
    "    # NAME_TYPE_SUITE\n",
    "    rare_list = [\"NAME_TYPE_SUITE\"]\n",
    "    rare_encoder(df, 0.01, rare_list)\n",
    "    \n",
    "    # NAME_EDUCATION_TYPE\n",
    "    # Akademik derecenin frekansı az olduğu için bununla yüksek eğitimi aynı sınıfa aldık.\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"] == \"Academic degree\",\n",
    "                                         \"Higher education\", df[\"NAME_EDUCATION_TYPE\"])\n",
    "    \n",
    "    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"].str.contains(\"Secondary / secondary special\"),\n",
    "                                         \"Secondary_secondary_special\", df[\"ORGANIZATION_TYPE\"])\n",
    "    \n",
    "    # NAME_FAMILY_STATUS\n",
    "    df[\"NAME_FAMILY_STATUS\"] = np.where(df[\"NAME_FAMILY_STATUS\"].str.contains(\"Single / not married\"),\n",
    "                                        \"Single_not_married\", df[\"NAME_FAMILY_STATUS\"])\n",
    "    \n",
    "    \n",
    "    # NAME_HOUSING_TYPE\n",
    "    df[\"NAME_HOUSING_TYPE\"] = np.where(df[\"NAME_HOUSING_TYPE\"].str.contains(\"House / apartment\"),\n",
    "                                       \"House_apartment\", df[\"NAME_HOUSING_TYPE\"])\n",
    "    \n",
    "    # NAME_TYPE_SUITE\n",
    "    df[\"NAME_TYPE_SUITE\"] = np.where(df[\"NAME_TYPE_SUITE\"].str.contains(\"Spouse, partner\"),\n",
    "                                       \"Spouse_partner\", df[\"NAME_TYPE_SUITE\"])\n",
    "    \n",
    "    \n",
    "    # NAME_CONTRACT_TYPE\n",
    "    # Kategorik olan ama cinsiyet gibi 0 ve 1 olarak kodlanacak değişkenlere binary encode yaptık.\n",
    "    for bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    \n",
    "    # WEEKDAY_APPR_PROCESS_START\n",
    "    # Gün isimlerini 1,2,3....,7 olarak değiştireceğiz.\n",
    "    # Daha sonra günler döngüsel yapıda oldukları için bunlara cycle encode uygulayacağız.\n",
    "    # Asıl değişkenleri silmedik feature importance da bak!\n",
    "    weekday_dict = {'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}\n",
    "    df.replace({'WEEKDAY_APPR_PROCESS_START': weekday_dict}, inplace=True)\n",
    "    # Cycle encode\n",
    "    df['NEW_WEEKDAY_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n",
    "    df[\"NEW_WEEKDAY_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n",
    "    \n",
    "    # HOUR_APPR_PROCESS_START\n",
    "    # değişken müşterinin hangi saatte başvurduğu bilgisini veriyordu.\n",
    "    # Saat bilgisi de yine döngüsel olduğu için buna da cycle encode yapıyoruz.\n",
    "    df['NEW_HOUR_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n",
    "    df[\"NEW_HOUR_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n",
    "  \n",
    "    ###########\n",
    "    # DROP\n",
    "    ###########\n",
    "    # FLAG_MOBIL ve FLAG_CONT_MOBILE değişkenlerinde iki alt sınıfı var ve birinin frekansı çok az.\n",
    "    # Yani bilgi taşımayan değişken bu nedenle drop ediyoruz.\n",
    "    drop_cols = [\"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n",
    "                 \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n",
    "    df.drop(drop_cols, axis = 1, inplace = True)\n",
    "    \n",
    "    # OBS_30_CNT_SOCIAL_CIRCLE,OBS_60_CNT_SOCIAL_CIRCLE\n",
    "    df.drop(['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'], axis = 1, inplace = True)\n",
    "    \n",
    "    # REGION\n",
    "    # Bu değişkenler bölge ve şehir bazında ayrı ayrı puan veriyordu\n",
    "    # Biz bunları toplayarak tek bir değişken elde ettik ve diğerlerini dropladık.\n",
    "    cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \n",
    "            \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n",
    "    df[\"NEW_REGION\"] = df[cols].sum(axis = 1)\n",
    "    df.drop(cols, axis = 1, inplace = True)\n",
    "    \n",
    "    # Flag_DOCUMENT\n",
    "    # Bu değişkenler her dökümanın ayrı ayrı verilip verilmediği bilgisini veriyordu.\n",
    "    # Biz bunları toplayarak tek bir değişken elde ettik,yani totalde verilen belge sayısını hesapladık\n",
    "    # ve diğerlerini dropladık.\n",
    "    docs = [col for col in df.columns if 'FLAG_DOC' in col]\n",
    "    df['NEW_DOCUMENT'] = df[docs].sum(axis=1)\n",
    "    df.drop(docs, axis = 1, inplace = True)\n",
    "    \n",
    "    ##########################\n",
    "    # FEATURE ENGINEERING\n",
    "    ##########################\n",
    "    # 1. Müşteri başvurudan ne kadar önce işe başladı(gün) / müşterinin yaşı(gün)\n",
    "    df['NEW_DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # 2. Müşterinin toplam geliri / kredi tutarı\n",
    "    df['NEW_INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "\n",
    "    # 3. Müşterinin toplam geliri / aile üyesi süresi\n",
    "    # Ailede kişi başına ne kadar gelir var bunu ortaya çıkarıyor.\n",
    "    df['NEW_INCOME_PER_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "\n",
    "    # 4. Kredinin yıllık ödemesi / Müşterinin geliri\n",
    "    # Eğer bu değer 0-1 arasındaysa iyi yani geliri kredi ödemesinden fazla\n",
    "    # Eğer bu değer 1 ise kötü yani geliri kredi ödemesinden daha az.\n",
    "    df['NEW_ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "\n",
    "    # 5. Kredinin yıllık ödemesi / kredi tutarı\n",
    "    df['NEW_PAYMENT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    # 6. EXT_SOURCE değişkenleri dışarıdan alınan puanlardı.Ortalamaları ile yeni bir değişken oluşturduk.\n",
    "    df[\"NEW_EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "\n",
    "    # 7. Bu değişkenleri çarparak ağırlıklı yeni bir değişken oluşturduk.\n",
    "    df['NEW_EXTSOURCES_WPOINT'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    \n",
    "    # 8. Kredi ile satın alınacak malların fiyatı / toplam kredi tutarı\n",
    "    # Bu oran 0-1 arasında ise müşterinin alacağı mal fiyatından daha fazla kredi çekmesi demek.\n",
    "    # Bu oranın 1 olması demek müşterinin ihtiyacı kadar kredi çektiği anlamına gelir.\n",
    "    # Bu oran 1 den büykse müşteri ihtiyacından daha az kredi çekmiş demektir.\n",
    "    df[\"NEW_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
    "    \n",
    "    # 9.Yukarıdaki değişkenle bağlantılı olarak:\n",
    "    # Bu fark 0 dan büyükse ihtiyacından az kredi çekmiş\n",
    "    # 0 olursa ihtiyacı kadar çekmiş\n",
    "    # 0 dan küçükse ihtiyacından fazla çekmiş\n",
    "    df[\"NEW_GOODS_CREDIT_DIFF\"] = df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]\n",
    "    \n",
    "    # 10. (Kredi ile satın alınacak malların fiyatı / toplam kredi tutarı) / toplam gelir\n",
    "    df[\"NEW_GOODS_CREDIT_DIFF_RATIO\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
    "    \n",
    "    # 11. Toplam gelir / müşterinin yaşı(gün cinsinden)\n",
    "    df['NEW_INCOME_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # 12. Kişinin DAYS_BIRTH değişkeni gün cinsindn yaşını veriyordu.\n",
    "    # Ama değerler - (çünkü şu kadar gün önce doğmuş bilgisini veriyor.)\n",
    "    # Bu yüzden müşterinin yaşını bulmak için - ile çarpıp 360 a böleceğiz.\n",
    "    df[\"NEW_DAYS_BIRTH\"] = round(df[\"DAYS_BIRTH\"]* -1 / 365)\n",
    "    \n",
    "    # 13. Yaşlara göre müşterileri segmentlere ayırma\n",
    "    df.loc[df[\"NEW_DAYS_BIRTH\"] <= 34 ,\"NEW_SEGMENT_AGE\"] = \"Young\"\n",
    "    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 34)&(df[\"NEW_DAYS_BIRTH\"] <= 54) ,\"NEW_SEGMENT_AGE\"] = \"Middle_Age\"\n",
    "    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 54),\"NEW_SEGMENT_AGE\"] = \"Old\"\n",
    "    \n",
    "    # 14. Gelire göre müşterileri segmentlere ayırma\n",
    "    df.loc[df[\"AMT_INCOME_TOTAL\"] <= 112500 ,\"NEW_SEGMENT_INCOME\"] = \"Low_Income\"\n",
    "    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 112500)&(df[\"AMT_INCOME_TOTAL\"] <= 225000) ,\"NEW_SEGMENT_INCOME\"] = \"Middle_Income\"\n",
    "    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 225000),\"NEW_SEGMENT_INCOME\"] = \"High_Income\"\n",
    "    \n",
    "    # 15. Kişi başvuru yaparken kimle birlikteydi?\n",
    "    df.loc[df['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 0\n",
    "    df.loc[df['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 1\n",
    "    df.loc[df['NAME_TYPE_SUITE'].isnull(), 'NEW_TYPE_SUITE_CAT'] = np.nan\n",
    "    \n",
    "    # 16. Elimizde müşterinin çevresinde 30 ve 60 gün temerrüde düşmüş kişi sayısı bilgisini veren iki\n",
    "    # değişken vardı. Biz 30 ve 60 günü birleştirerek;\n",
    "    # müşterinin çevresinde temerrüde düşen varsa 1 etiketlesin\n",
    "    # temerrüde düşen yoksa 0 etiketlesin dedik.\n",
    "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n",
    "           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n",
    "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n",
    "           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n",
    "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n",
    "           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n",
    "    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n",
    "           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 0\n",
    "    \n",
    "    ########################\n",
    "    # One-Hot Encoding\n",
    "    ########################\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
    "    \n",
    "    # Dropping feature named index\n",
    "    df.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    #print(df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "\n",
    "###############################\n",
    "# Bureau and Bureau Balance\n",
    "##############################\n",
    "\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\", nrows = num_rows)\n",
    "    bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\n",
    "    #cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "    df,df_cat=one_hot_encoder(df)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    agg_list = {'MONTHS_BALANCE': ['min', 'max', 'size'] }\n",
    "    for col in df_cat:\n",
    "        agg_list[col] = ['mean','sum']\n",
    "\n",
    "    bb_agg = df.groupby(\"SK_ID_BUREAU\").agg(agg_list)\n",
    "    # Degisken isimlerinin yeniden adlandirilmasi \n",
    "    bb_agg.columns = pd.Index([col[0] + \"_\" + col[1].upper() for col in bb_agg.columns.tolist()])\n",
    "    # New feature\n",
    "    bb_agg['NEW_STATUS_SCORE'] = bb_agg['STATUS_1_SUM'] + bb_agg['STATUS_2_SUM']^2 + bb_agg['STATUS_3_SUM']^3 + bb_agg['STATUS_4_SUM']^4 + bb_agg['STATUS_5_SUM']^5\n",
    "    \n",
    "    bureau_and_bb = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del df, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n",
    "    rare_encoder(bureau_and_bb,0.2,cat_cols)\n",
    "    \n",
    "    #CREDIT_ACTIVE degiskeninin sinif sayisini 2'ye düşürdük (Closed-Active)\n",
    "    bureau_and_bb['CREDIT_ACTIVE'] = bureau_and_bb['CREDIT_ACTIVE'].replace(\"Rare\", 'Active')\n",
    "    #CREDIT_CURRENCY değişkenin 1 sınıfı, veri setinin %99'unu kapladığı için yani dengesiz veri olduğu için anlamsız bilgi taşır.Bu yüzden çıkaracağız.\n",
    "    bureau_and_bb.drop(\"CREDIT_CURRENCY\", inplace = True, axis = 1)\n",
    "    \n",
    "    #######################\n",
    "    # FEATURE ENGINEERING\n",
    "    #######################\n",
    "    # 1. Active ve Closed Krediler için kredi erken kapanmışmı? \n",
    "    # Eğer kredi durumu aktifse ve DAYS_CREDIT_ENDDATE değişkeni 0 dan küçükse(yani - olması\n",
    "    # o kadar gün önce sona erdiği anlamına gelir.) bu kişi kredisini erken kapatmıştır.\n",
    "    # Closed da da kredisi kapanmış ama erken ödemesi erken bitmiş olanlar var.\n",
    "    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Active') & (bureau_and_bb['DAYS_CREDIT_ENDDATE'] < 0), 'NEW_EARLY_ACTİVE'] = 1\n",
    "    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Closed') & (abs(bureau_and_bb['DAYS_CREDIT_ENDDATE']) < abs(bureau_and_bb['DAYS_ENDDATE_FACT']) ), 'NEW_EARLY_CLOSED'] = 1\n",
    "    \n",
    "    # 2. Uzatılmış Kredilerin 1 ile değiştirilmesi\n",
    "    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb.loc[:,'CNT_CREDIT_PROLONG']\n",
    "    prolong = [1,2,3,4,5,6,7,8,9]\n",
    "    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb['NEW_CNT_CREDIT_PROLONG_CAT'].replace(prolong, 1)\n",
    "    \n",
    "    # 3. Kişi Kaç farklı kredi tipi almış\n",
    "    temp_bu = bureau_and_bb[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n",
    "    bureau_and_bb = bureau_and_bb.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n",
    "    \n",
    "    # 4. Borç Oranı\n",
    "    # Kredi Bürosuna olan mevcut borç / kredi bürosu için mevcut kredi \n",
    "    # 1 le toplamamızın sebebi tanımsızlık olmasın diye\n",
    "    bureau_and_bb['NEW_DEPT_RATIO'] = bureau_and_bb['AMT_CREDIT_SUM_DEBT'] / (bureau_and_bb['AMT_CREDIT_SUM']+1)\n",
    "\n",
    "    # 5.Kredi güncellenmesi yenimi ?\n",
    "    # 90 günü baz aldık. Çünkü bankalarda 3 ay gecikmeden sonra işlem başlatılıyor.\n",
    "    bureau_and_bb['NEWS_DAYS_CREDIT_UPDATE'] = bureau_and_bb['DAYS_CREDIT_UPDATE'].apply(lambda x : 'old' if x < -90 else 'new')\n",
    "    #cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n",
    "    \n",
    "    ###########################\n",
    "    # One-Hot Encoding\n",
    "    ###########################\n",
    "    bureau_and_bb, bureau_and_bb_cat = one_hot_encoder(bureau_and_bb)\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "                        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "                        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "                        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "                        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n",
    "                        'NEW_STATUS_SCORE':['min','mean','max'],\n",
    "                        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "                        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "                        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "                        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "                        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "                        'AMT_ANNUITY': ['max', 'mean'],\n",
    "                        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "                        'MONTHS_BALANCE_MIN': ['min'],\n",
    "                        'MONTHS_BALANCE_MAX': ['max'],\n",
    "                        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "                        'NEW_DEPT_RATIO': ['min','max','mean'] }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_and_bb_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in df_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau_and_bb.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    \n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    \n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    #print(bureau_agg.columns.tolist())\n",
    "    return bureau_agg\n",
    "\n",
    "###########################\n",
    "# Previous_application\n",
    "###########################\n",
    "\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(\"../input/home-credit-default-risk/previous_application.csv\", nrows = num_rows)\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "    \n",
    "    \n",
    "    # Nümerik değişkenleri incelerken bazı değişkenlerin max değerinin gerçek dışı \n",
    "    # yani çok büyük olduğunu gördük.365 e böldük 1000 yıl gibi bir değer geldi.\n",
    "    # Bu sebeple bu kolondaki böyle büyük değerlere Nan değeri atayacağım\n",
    "    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    \n",
    "    # Bazı kategorik değişkenlerin alt sınıfları XNA, XAP olarak yazılmış aslında bunlar nan değer.\n",
    "    # Bu yüzden bunlara nan atıyoruz.\n",
    "    na = ['XNA', 'XAP']\n",
    "    for col in cat_cols:\n",
    "        for n in na:\n",
    "            df.loc[df[col] == n, col] = np.nan\n",
    "\n",
    "    #########\n",
    "    # RARE\n",
    "    ########\n",
    "    rare_encoder(df, 0.01,cat_cols)\n",
    "    \n",
    "    # NAME_GOODS_CATEGORY\n",
    "    # Bu değişkenin bazı alt sınıflarını others da topladık.\n",
    "    a = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure', \n",
    "         'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness', 'Additional Service', \n",
    "         'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals'] \n",
    "\n",
    "    df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(a, 'others')\n",
    "    \n",
    "    # \"NAME_SELLER_INDUSTRY\n",
    "    df[\"NAME_SELLER_INDUSTRY\"] = df[\"NAME_SELLER_INDUSTRY\"].replace(\"Rare\", 'others')\n",
    "    \n",
    "    # PREV_NAME_GOODS_CATEGORY\n",
    "    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Photo / Cinema Equipment\"),\n",
    "                                       \"Photo_Cinema_Equipment\", df[\"NAME_GOODS_CATEGORY\"])\n",
    "    \n",
    "    # CHANNEL_TYPE\n",
    "    df[\"CHANNEL_TYPE\"] = np.where(df[\"CHANNEL_TYPE\"].str.contains(\"Regional / Local\"),\n",
    "                                       \"Regional_Local\", df[\"CHANNEL_TYPE\"])\n",
    "    \n",
    "    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Audio/Video\"),\n",
    "                                       \"Audio_Video\", df[\"NAME_GOODS_CATEGORY\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############\n",
    "    # DROP\n",
    "    ############\n",
    "    # %99 eksik veri olan ve değişkenleri incelediğimizde bilgi taşımadığını düşündüğümüz \n",
    "    # değişkenleri verisetinden çıkarıyoruz.\n",
    "\n",
    "    del_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n",
    "                'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n",
    "                'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n",
    "\n",
    "    df.drop(del_cols, axis=1, inplace=True)\n",
    "    \n",
    "    #########################\n",
    "    # FEATURE ENGINEERING\n",
    "    #########################\n",
    "    \n",
    "    # 1. \"HOUR_APPR_PROCESS_START\" (Müşteri önceki başvurusu için yaklaşık olarak hangi saatte başvurdu) \n",
    "    # değişkenin NEW_WORK_HOURS ve NEW_OFF_HOURS olarak iki sınıfa ayrılması\n",
    "    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"]= df.loc[:,\"HOUR_APPR_PROCESS_START\"]\n",
    "    a = [8,9,10,11,12,13,14,15,16,17]\n",
    "    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(a, \"WORK_HOURS\")\n",
    "    b = [18,19,20,21,22,23,0,1,2,3,4,5,6,7]\n",
    "    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(b, 'OFF_HOURS')\n",
    "    \n",
    "    # 2. \"WEEKDAY_APPR_PROCESS_START\"  değişkeninin  WEEK_DAY ve WEEKEND olarak iki sınıfa  indirdik\n",
    "    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df.loc[:,\"WEEKDAY_APPR_PROCESS_START\"]\n",
    "    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['MONDAY','TUESDAY', 'WEDNESDAY','THURSDAY','FRIDAY'], 'WEEK_DAY')\n",
    "    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['SATURDAY', 'SUNDAY'], 'WEEKEND')\n",
    "    \n",
    "    # 3. \"NAME_TYPE_SUITE\"  değişkeninin single ve multiple olarak iki kategoriye ayrılması\n",
    "    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace('Unaccompanied', 'single')\n",
    "    b = ['Family', 'Spouse, partner', 'Children', 'Other_B', 'Other_A', 'Group of people']\n",
    "    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(b, 'multiple')\n",
    "    \n",
    "    # 4. Müşteri öncek başvuruda ne kadar kredi istedi / önceki başvurunun nihai kredi tutarı\n",
    "    df[\"NEW_AMT_CREDIT_RATIO\"] = df[\"AMT_APPLICATION\"]/df[\"AMT_CREDIT\"]\n",
    "    # 1 e yakın olması istediği krediden az almış olması\n",
    "    # 0 a yakın olması istediği krediden fazla almış olması\n",
    "    # 1 ise istenilen kredi ile verilen kredi miktarı aynı olması\n",
    "    \n",
    "    # 5. x <= 1 ise istediği krediyi almış veya daha fazlasını almış.\n",
    "    df[\"NEWX2_FLAG_AMT_CREDIT_RATIO\"] = df[\"NEW_AMT_CREDIT_RATIO\"].apply(lambda x: 1 if(x<=1) else 0)\n",
    "    \n",
    "    # 6. NFLAG_INSURED_ON_APPROVAL değişkeni yerine kullanılmak izere NEW_INSURANCE değişkeni tanımlandı.\n",
    "    # NFLAG_INSURED_ON_APPROVAL: Müşteri önceki başvuru sırasında sigorta talep etti mi?\n",
    "    # AMT_CREDIT: Bankanın verdiği nihai kredi tutarı\n",
    "    # AMT_GOOD_PRICE: Müşterinin(varsa) önceki uygulamada istediği malın fiyatı\n",
    "    ########### NAN SAYISINI AZALTMAK İÇİN YAPTIK ÖNEMLİİİ !!!!!!  ############\n",
    "    df[(df['AMT_CREDIT'] == 0) | (df['AMT_GOODS_PRICE'] == 0)]['NEW_INSURED_ON_APPROVAL'] = np.nan\n",
    "    df['INSURANCE_AMOUNT'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n",
    "    df['NEW_INSURED_ON_APPROVAL'] = df['INSURANCE_AMOUNT'].apply(lambda x: 1 if x > 0 else (0 if x <= 0 else np.nan))\n",
    "    df.drop('INSURANCE_AMOUNT', axis=1, inplace=True)\n",
    "    \n",
    "    # 7. Kaç yılda ödedi = Bankanın verdiği kredi / yıllık taksit tutarı\n",
    "    df['NEW_HOW_PAID_YEARS'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    \n",
    "    # 8. Başvurduğu kredi miktarı / almak istediği ürünün fiyatı \n",
    "    df['NEW_GOODS_RATIO'] = df['AMT_APPLICATION'] / df['AMT_GOODS_PRICE']\n",
    "    \n",
    "    #  Ödeme gününü geciktirmiş mi bunu gösteren değişken türetelim.\n",
    "    # 1= geciktirmiş, 0 = geciktirmemiş, NaN = boş değer\n",
    "    # (Mevcut başvurunun başvuru tarihine göre, bir önceki başvurunun ilk vadesi ne zamandı?) - \n",
    "    # (Mevcut başvurunun başvuru tarihine göre, bir önceki başvurunun ilk vadesi ne zaman olmalıydı?)\n",
    "    # 9. NÜMERİK OLAN\n",
    "    df['NEW_LATE_DAYS'] =  df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_FIRST_DUE'] \n",
    "    # 10 .SINIFLI OLAN\n",
    "    k = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_LAST_DUE\"]\n",
    "    df[\"NEW_FLAG_LATE_DAYS\"] = [1 if i >= 0 else (0 if i < 0  else \"NaN\") for i in k]\n",
    "    \n",
    "    # WEEKDAY_APPR_PROCESS_START_DIC\n",
    "    # Cycle encoding gün, ay, yıl gibi döngüsel değişkenlerde kullanılabilir.\n",
    "    df['WEEKDAY_APPR_PROCESS_START_DIC'] = df['WEEKDAY_APPR_PROCESS_START'].map({\n",
    "        'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7})\n",
    "    df['NEW_WEEKDAY_SIN'] = np.sin(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n",
    "    df['NEW_WEEKDAY_COS'] = np.cos(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n",
    "    # df.drop('WEEKDAY_APPR_PROCESS_START', axis=1, inplace=True) # feature imp bak!!!!!\n",
    "    \n",
    "    #######################\n",
    "    # One_Hot Encoding\n",
    "    #######################\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n",
    "\n",
    "    #################\n",
    "    # Aggregation\n",
    "    #################\n",
    "    col_list = df.columns.tolist()\n",
    "    id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n",
    "    num_list = [col for col in col_list if col not in cat_cols + id_list]\n",
    "    \n",
    "    # Previous applications numeric features\n",
    "    agg_num_prev = {}\n",
    "    for num in num_list:\n",
    "        agg_num_prev[num] = ['min', 'max', 'mean', 'median']\n",
    "        \n",
    "    # Previous applications categorical features\n",
    "    agg_cat_prev = {}\n",
    "    for cat in cat_cols:\n",
    "        agg_cat_prev[cat] = ['mean']\n",
    "        \n",
    "    prev_agg = df.groupby('SK_ID_CURR').agg({**agg_num_prev, **agg_cat_prev})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(agg_num_prev)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(agg_num_prev)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del refused, refused_agg, approved, approved_agg, df\n",
    "    gc.collect()\n",
    "    #print(prev_agg.columns.tolist())\n",
    "    return prev_agg\n",
    "\n",
    "##################\n",
    "# Pos_cash\n",
    "#################\n",
    "\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "    \n",
    "    # Rare\n",
    "    rare_encoder(df, 0.01, cat_cols)\n",
    "    # One-Hot Encoding\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n",
    "    \n",
    "    # Numerical Features\n",
    "    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "                    'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n",
    "                    'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n",
    "                    'SK_DPD': ['max', 'mean'],\n",
    "                    'SK_DPD_DEF': ['max', 'mean']\n",
    "                   }\n",
    "    \n",
    "    # Categorical Features\n",
    "    original_columns = list(df.columns)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    for cat in new_columns:\n",
    "        aggregations[cat] = ['mean']\n",
    "        \n",
    "    pos_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    del df\n",
    "    gc.collect()\n",
    "    #print(pos_agg.columns.tolist())\n",
    "    return pos_agg\n",
    "\n",
    "###############################\n",
    "# Installments_payments\n",
    "###############################\n",
    "\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv', nrows = num_rows)\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category= True) ### neden cat var?\n",
    "    \n",
    "    #########################\n",
    "    # Feature Engineering\n",
    "    #########################\n",
    "    # Her kredi taksidi ödemesinde ödediği miktarla aslı arasındaki fark ve bunun yüzdesi\n",
    "    df['PAYMENT_PERC'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n",
    "    df['PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n",
    "\n",
    "    # Vadesi geçmiş günler ve vadesinden önceki günler -- sadece pozitif değerler alınır\n",
    "    df['DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n",
    "    df['DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n",
    "    df['DPD'] = df['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    df['DBD'] = df['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # Her bir taksit ödemesinin gec olup olmama durumu 1: gec ödedi 0: erken ödemeyi temsil eder\n",
    "    df['NEW_DAYS_PAID_EARLIER'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n",
    "    df['NEW_NUM_PAID_LATER'] = df['NEW_DAYS_PAID_EARLIER'].map(lambda x: 1 if x<0 else 0)\n",
    "    \n",
    "    # Numeric Features\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    \n",
    "    # Categorical Features\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    del df\n",
    "    gc.collect()\n",
    "    #print(ins_agg.columns.tolist())\n",
    "    return ins_agg\n",
    "\n",
    "\n",
    "#########################\n",
    "# Credit_card_balance\n",
    "########################\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv', nrows = num_rows)\n",
    "    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
    "    \n",
    "    # Rare\n",
    "    df[\"NAME_CONTRACT_STATUS\"] = np.where(~(df[\"NAME_CONTRACT_STATUS\"].isin([\"Active\", \"Completed\"])),\n",
    "                                          \"Rare\", df[\"NAME_CONTRACT_STATUS\"])\n",
    "    \n",
    "    # One Hot Encoder\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n",
    "    \n",
    "    ########################\n",
    "    # Feature Engineering\n",
    "    ########################\n",
    "    # ATM den cekilen tutar + mal satın alma miktari\n",
    "    df[\"TOTAL_SPENDING\"] = df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"]\n",
    "    # Müşterinin ay boyunca ödediği para - aylık asgari taksit\n",
    "    df[\"REGULARITY_PAYMENT\"] = df[\"AMT_INST_MIN_REGULARITY\"] - df[\"AMT_PAYMENT_TOTAL_CURRENT\"]\n",
    "    \n",
    "    # General aggregations\n",
    "    df.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = df.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return cc_agg\n",
    "\n",
    "#################\n",
    "# MODEL\n",
    "#################\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            boosting_type= 'goss',\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.005134,\n",
    "            num_leaves=54,\n",
    "            colsample_bytree=0.508716,\n",
    "            subsample=1,\n",
    "            subsample_for_bin= 240000,\n",
    "            max_depth=10,\n",
    "            reg_alpha=0.436193,\n",
    "            reg_lambda=0.436193,\n",
    "            min_split_gain=0.024766,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(20, 25))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('lgbm_importances01.png')\n",
    "    \n",
    "\n",
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    print(\"application shape:\", df.shape)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13941.850354,
   "end_time": "2021-09-06T20:01:46.004799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-06T16:09:24.154445",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}